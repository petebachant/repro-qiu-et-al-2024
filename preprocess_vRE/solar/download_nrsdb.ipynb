{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states=['Maine','Massachusetts','Connecticut','New Hampshire','Rhode Island','Vermont']\n",
    "hdir=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get the list of poitns within each state\n",
    "import h5pyd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "f = h5pyd.File(\"/nrel/nsrdb/v3/nsrdb_2012.h5\", 'r')\n",
    "meta = pd.DataFrame(f['meta'][...])\n",
    "states_b=[b'Maine',b'Massachusetts',b'Connecticut',b'New Hampshire',b'Rhode Island',b'Vermont']\n",
    "states=['Maine','Massachusetts','Connecticut','New Hampshire','Rhode Island','Vermont']\n",
    "for ist in range(len(states_b)):\n",
    "    data = meta.loc[meta['state'] == states_b[ist]] # Note .h5 saves strings as bit-strings\n",
    "    latlon=data[['latitude','longitude']]\n",
    "    latlon.to_csv(f'{states[ist]}_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_json_and_handle_errors(response: requests.Response) -> dict:\n",
    "    if response.status_code != 200:\n",
    "        print(f\"An error has occurred with the server or the request. The request response code/status: {response.status_code} {response.reason}\")\n",
    "        print(f\"The response body: {response.text}\")\n",
    "        exit(1)\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "    except:\n",
    "        print(f\"The response couldn't be parsed as JSON, likely an issue with the server, here is the text: {response.text}\")\n",
    "        exit(1)\n",
    "    if len(response_json['errors']) > 0:\n",
    "        errors = '\\n'.join(response_json['errors'])\n",
    "        print(f\"The request errored out, here are the errors: {errors}\")\n",
    "        exit(1)\n",
    "    return response_json\n",
    "\n",
    "API_KEY = \"\"\n",
    "EMAIL = \"\"\n",
    "BASE_URL = \"https://developer.nrel.gov/api/nsrdb/v2/solar/psm3-download.json?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10208\n"
     ]
    }
   ],
   "source": [
    "#put the points together\n",
    "npoint_per_request=50\n",
    "indexes=[]\n",
    "for state in states:\n",
    "    df=pd.read_csv(f'./points_NSRDB/{state}_lonlat.csv',index_col=0).index.to_list()\n",
    "    indexes.extend(df)\n",
    "npointst=len(indexes) \n",
    "nstack=npointst//npoint_per_request+1 \n",
    "print(nstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the points together\n",
    "npoint_per_request=50\n",
    "indexesa=pd.DataFrame()\n",
    "for state in states:\n",
    "    dfa=pd.read_csv(f'./points_NSRDB/{state}_lonlat.csv',index_col=0)\n",
    "    indexesa=pd.concat([indexesa,dfa])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_para = {\n",
    "        'attributes': 'dhi,dni,ghi,dew_point,wind_speed,air_temperature,surface_pressure',\n",
    "        'interval': '60',\n",
    "        'to_utc': 'false',\n",
    "        'half_hour': 'false',\n",
    "        'include_leap_day': 'true',\n",
    "        'api_key': API_KEY,\n",
    "        'email':'liying_95@126.com'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls=[]\n",
    "npointst=len(indexes) \n",
    "nstack=npointst//npoint_per_request+1 \n",
    "sy=2007\n",
    "ey=2007\n",
    "for yr in range(sy,ey+1):\n",
    "    print(f\"Processing name: {str(yr)}\")\n",
    "    for id in range(0,nstack):\n",
    "        #aPOINTS = indexes[0:1] #this is one point for testing\n",
    "        if id!=nstack-1:\n",
    "            aPOINTS = indexes[id*npoint_per_request:(id+1)*npoint_per_request]\n",
    "        else:\n",
    "            aPOINTS = indexes[id*npoint_per_request:]\n",
    "        POINTS = ', '.join(str(num) for num in aPOINTS)\n",
    "        print(id)\n",
    "        input_data=input_data_para.copy()\n",
    "        input_data['names'] = [str(yr)]\n",
    "        input_data['location_ids'] = POINTS\n",
    "        #print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
    "\n",
    "        if '.csv' in BASE_URL:\n",
    "            url = BASE_URL + urllib.parse.urlencode(data, True)\n",
    "            # Note: CSV format is only supported for single point requests\n",
    "            data = pd.read_csv(url)\n",
    "            #print(f'Response data (you should replace this print statement with your processing): {data}')\n",
    "            # You can use the following code to write it to a file\n",
    "            # data.to_csv('SingleBigDataPoint.csv')\n",
    "        else:\n",
    "            headers = {'x-api-key': API_KEY}\n",
    "            data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
    "            download_url = data['outputs']['downloadUrl']\n",
    "            # You can do with what you will the download url\n",
    "            #print(data['outputs']['message'])\n",
    "            #print(f\"Data can be downloaded from this url when ready: {download_url}\")\n",
    "\n",
    "            # Delay for 3 second to prevent rate limiting\n",
    "            time.sleep(3)\n",
    "        #print(f'Processed')\n",
    "        urls.append(download_url)\n",
    "        with open(f'./urls/urls_{yr}.txt', 'a') as f:\n",
    "            f.write(download_url + '\\n')\n",
    "        if id!=0:\n",
    "            if id%19==0:\n",
    "                time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#check if all files are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11953\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "yr=2017\n",
    "leftlist=[]\n",
    "for index in indexes:\n",
    "    filename=f'{hdir}/Meteorological_Data/High/Solar/{yr}/{index}*.csv'\n",
    "    matching_files = glob.glob(filename)\n",
    "    if not matching_files:\n",
    "        leftlist.append(index)\n",
    "print(len(leftlist))\n",
    "npointst=len(leftlist) \n",
    "nstack=npointst//npoint_per_request+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls=[]\n",
    "npointst=len(leftlist) \n",
    "nstack=npointst//npoint_per_request+1 \n",
    "for id in range(0,nstack):\n",
    "    if id!=nstack-1:\n",
    "        aPOINTS = leftlist[id*npoint_per_request:(id+1)*npoint_per_request]\n",
    "    else:\n",
    "        aPOINTS = leftlist[id*npoint_per_request:]\n",
    "    POINTS = ', '.join(str(num) for num in aPOINTS)\n",
    "    input_data=input_data_para.copy()\n",
    "    input_data['names'] = [str(yr)]\n",
    "    input_data['location_ids'] = POINTS\n",
    "    if '.csv' in BASE_URL:\n",
    "        url = BASE_URL + urllib.parse.urlencode(data, True)\n",
    "        data = pd.read_csv(url)\n",
    "    else:\n",
    "        headers = {'x-api-key': API_KEY}\n",
    "        data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
    "        download_url = data['outputs']['downloadUrl']\n",
    "        # Delay for 1 second to prevent rate limiting\n",
    "        time.sleep(3)\n",
    "    urls.append(download_url)\n",
    "    with open(f'./urls/left_{yr}.txt', 'a') as f:\n",
    "        for item in urls:\n",
    "            f.write(item + '\\n')  # Write the item and move to the next line    \n",
    "    if id!=0:\n",
    "        if id%19==0:\n",
    "            time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
